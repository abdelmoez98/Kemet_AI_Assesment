{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq encoder decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 05:12:22.770053: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-23 05:12:22.819946: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-23 05:12:22.819986: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-23 05:12:22.822142: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-23 05:12:22.829420: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-23 05:12:22.830226: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-23 05:12:23.896057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_sequences(sequences, vocabulary_size):\n",
    "    \"\"\"\n",
    "    One-hot encode a list of sequences.\n",
    "\n",
    "    Args:\n",
    "    sequences (list of lists): List of sequences, where each sequence is a list of integers representing tokens.\n",
    "    vocabulary_size (int): Size of the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "    list of numpy arrays: List of one-hot encoded matrices.\n",
    "    \"\"\"\n",
    "    one_hot_matrices = []\n",
    "    for sequence in sequences:\n",
    "        one_hot_matrix = np.zeros((len(sequence), vocabulary_size), dtype=np.int32)\n",
    "        for i, token_id in enumerate(sequence):\n",
    "            one_hot_matrix[i, token_id] = 1\n",
    "        one_hot_matrices.append(one_hot_matrix)\n",
    "    return np.array(one_hot_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('translation_train.csv')\n",
    "arabic_texts = df['Arabic'].values\n",
    "english_texts = df['English'].values\n",
    "english_texts = ['<BOS> '+s+' <EOS>' for s in english_texts]\n",
    "decoder_output_size = 1000\n",
    "\n",
    "# Tokenize the text\n",
    "ar_tokenizer = Tokenizer()\n",
    "ar_tokenizer.fit_on_texts(arabic_texts)\n",
    "en_tokenizer = Tokenizer(num_words=decoder_output_size)\n",
    "en_tokenizer.fit_on_texts(english_texts)\n",
    "arabic_sequences = ar_tokenizer.texts_to_sequences(arabic_texts)\n",
    "english_sequences = en_tokenizer.texts_to_sequences(english_texts)\n",
    "indicies = [i for i in range(len(arabic_sequences))  if len(arabic_sequences[i])<40 & len(english_sequences[i])<38]\n",
    "# Pad the sequences\n",
    "max_sequence_length = 40#max(max(len(seq) for seq in arabic_sequences), max(len(seq) for seq in english_sequences))\n",
    "arabic_sequences = pad_sequences([arabic_sequences[i] for i in indicies], maxlen=max_sequence_length, padding='post')\n",
    "english_sequences = pad_sequences([english_sequences[i] for i in indicies], maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "decoder_input_data = []\n",
    "decoder_target_data = []\n",
    "for i in range(english_sequences.shape[0]):\n",
    "    decoder_input_data.append(english_sequences[i][:-1].tolist())\n",
    "    decoder_target_data.append(english_sequences[i][1:].tolist())\n",
    "\n",
    "\n",
    "encoder_input_data = arabic_sequences\n",
    "decoder_input_data = np.array(decoder_input_data)\n",
    "decoder_target_data = one_hot_encode_sequences(decoder_target_data,decoder_output_size)\n",
    "# Split the data\n",
    "arabic_train, arabic_val, english_train, english_val, decoder_input_train, decoder_input_val, decoder_target_train, decoder_target_val = train_test_split(arabic_sequences, english_sequences, decoder_input_data, decoder_target_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ArbEngVec model\n",
    "arbengvec_model_path = 'randshuffle_5window_skipgram_300size.model'\n",
    "arbengvec_model = gensim.models.KeyedVectors.load(arbengvec_model_path).wv\n",
    "\n",
    "# Define the vocabulary size and embedding dimension\n",
    "vocab_size = len(arbengvec_model.key_to_index) + 1  # Plus 1 for the padding token\n",
    "embedding_dim = arbengvec_model.vector_size\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in arbengvec_model.key_to_index.items():\n",
    "    embedding_vector = arbengvec_model[word]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Create the embedding layer\n",
    "embedding_layer = Embedding(input_dim=vocab_size,\n",
    "                            output_dim=embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)  # Set trainable to False to keep the embeddings fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 39)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 40)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     multiple                     2317617   ['input_3[0][0]',             \n",
      "                                                          00         'input_4[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)               [(None, 300),                721200    ['embedding_1[0][0]']         \n",
      "                              (None, 300),                                                        \n",
      "                              (None, 300)]                                                        \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)               [(None, 39, 300),            721200    ['embedding_1[1][0]',         \n",
      "                              (None, 300),                           'lstm_2[0][1]',              \n",
      "                              (None, 300)]                           'lstm_2[0][2]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 39, 1000)             301000    ['lstm_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 233505100 (890.75 MB)\n",
      "Trainable params: 1743400 (6.65 MB)\n",
      "Non-trainable params: 231761700 (884.10 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(max_sequence_length,))\n",
    "x = embedding_layer(encoder_inputs)\n",
    "x, state_h, state_c = LSTM(embedding_dim,\n",
    "                           return_state=True)(x)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(max_sequence_length-1,))\n",
    "x = embedding_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(300, return_sequences=True, return_state=True)\n",
    "x,_,_ = decoder_lstm(x, initial_state=encoder_states)\n",
    "\n",
    "# Add a Dense layer with 224 units to further reduce the shape to (None, 224)\n",
    "decoder_outputs = Dense(decoder_output_size, activation='softmax')\n",
    "output = decoder_outputs(x)\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], output)\n",
    "\n",
    "# Compile & run training\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# Note that `decoder_target_data` needs to be one-hot encoded,\n",
    "# rather than sequences of integers like `decoder_input_data`!\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.7122 - val_loss: 1.0816\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.7099 - val_loss: 1.0830\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.7051 - val_loss: 1.0815\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.7025 - val_loss: 1.0829\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.6996 - val_loss: 1.0835\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6962 - val_loss: 1.0857\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6952 - val_loss: 1.0814\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6921 - val_loss: 1.0853\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.6896 - val_loss: 1.0835\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.6862 - val_loss: 1.0844\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.6839 - val_loss: 1.0883\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.6800 - val_loss: 1.0841\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.6767 - val_loss: 1.0851\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.6734 - val_loss: 1.0841\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6701 - val_loss: 1.0914\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6709 - val_loss: 1.0856\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6660 - val_loss: 1.0856\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6629 - val_loss: 1.0864\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.6602 - val_loss: 1.0881\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6565 - val_loss: 1.0885\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.6526 - val_loss: 1.0917\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.6499 - val_loss: 1.0895\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6472 - val_loss: 1.0910\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6453 - val_loss: 1.0911\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6421 - val_loss: 1.0936\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.6393 - val_loss: 1.0924\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6365 - val_loss: 1.0934\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.6333 - val_loss: 1.0928\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.6308 - val_loss: 1.0931\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6280 - val_loss: 1.0922\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6252 - val_loss: 1.0921\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.6227 - val_loss: 1.0929\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.6239 - val_loss: 1.0964\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.6235 - val_loss: 1.0979\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.6189 - val_loss: 1.0910\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.6135 - val_loss: 1.0934\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6107 - val_loss: 1.0945\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6075 - val_loss: 1.0942\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.6049 - val_loss: 1.0943\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.6021 - val_loss: 1.0984\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.5984 - val_loss: 1.0963\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.5955 - val_loss: 1.0969\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.5918 - val_loss: 1.0986\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.5884 - val_loss: 1.0974\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.5854 - val_loss: 1.1045\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.5844 - val_loss: 1.1018\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.5814 - val_loss: 1.1057\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.5793 - val_loss: 1.1024\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.5766 - val_loss: 1.1047\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.5721 - val_loss: 1.1033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f8dbf0d1030>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([arabic_train, decoder_input_train], decoder_target_train,\n",
    "          batch_size=512,\n",
    "          epochs=50,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'KerasTensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m decoded, state_h, state_c \u001b[38;5;241m=\u001b[39m decoder_lstm(\n\u001b[1;32m      7\u001b[0m     embedding_layer(decoder_inputs), initial_state\u001b[38;5;241m=\u001b[39mdecoder_states_inputs)\n\u001b[1;32m      8\u001b[0m decoder_states \u001b[38;5;241m=\u001b[39m [state_h, state_c]\n\u001b[0;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m decoder_model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mModel(\n\u001b[1;32m     11\u001b[0m     [decoder_inputs] \u001b[38;5;241m+\u001b[39m decoder_states_inputs,\n\u001b[1;32m     12\u001b[0m     [outputs] \u001b[38;5;241m+\u001b[39m decoder_states)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'KerasTensor' object is not callable"
     ]
    }
   ],
   "source": [
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(batch_shape=(300,None))\n",
    "decoder_state_input_c = Input(batch_shape=(300,None))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoded, state_h, state_c = decoder_lstm(\n",
    "    embedding_layer(decoder_inputs), initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "outputs = decoder_outputs(decoded)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [outputs] + decoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(np.array([input_seq]))\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    sequence = [1]\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [pad_sequences([sequence],maxlen=max_sequence_length-1,padding='post')] + states_value)\n",
    "        # Sample a token\n",
    "        token = np.argmax(output_tokens[0][len(sequence)-1])\n",
    "        sequence.append(token)\n",
    "        \n",
    "        if (token == 2 or\n",
    "           len(sequence) > max_sequence_length-1):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 355ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted_tokens = list(map(decode_sequence,arabic_val[:5]))\n",
    "predicted_text = en_tokenizer.sequences_to_texts(predicted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = [x.split() for x in en_tokenizer.sequences_to_texts(english_val[:5])]\n",
    "pred = [x.split() for x in predicted_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11824324324324323"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(map(single_meteor_score,ref,pred))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bos tom', 'bos the', 'bos what', 'bos the', 'bos i']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bos tom the in front of house eos',\n",
       " 'bos is this a picture that you eos',\n",
       " 'bos she put down her on paper eos',\n",
       " 'bos is the most beautiful city in eos',\n",
       " 'bos she took a to the hospital eos']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokenizer.sequences_to_texts(english_val[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need more training and powerful system to utilize the complete vocab size and sequence length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
